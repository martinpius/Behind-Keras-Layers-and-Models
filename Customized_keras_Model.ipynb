{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Customized keras Model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMapVsnCYkM98Ef65NcLzeG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinpius/Behind-Keras-Layers-and-Models/blob/main/Customized_keras_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrT7BfAeAYrP",
        "outputId": "757252ea-21f4-4862-81a6-2764beac6c2d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount = True)\n",
        "try:\n",
        "  COLAB = True\n",
        "  import tensorflow as tf\n",
        "  print(f\"You are on CoLab with tensorflow version: {tf.__version__}\")\n",
        "except Exception as e:\n",
        "  print(f\"{type(e)}: {e}\\n...please load your drive...\")\n",
        "  COLAB = False\n",
        "def time_fmt(x:float = 123.4987)->float:\n",
        "  h = int(x / (60 * 60))\n",
        "  m = int(x % (60 * 60) / 60)\n",
        "  s = int(x % 60)\n",
        "  return f\"{h}: {m:>02}: {s:>05.2f}\"\n",
        "print(f\"...time testing...time testing...time testing...\\ntime elapse: <<<{time_fmt()}>>>\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "You are on CoLab with tensorflow version: 2.4.1\n",
            "...time testing...time testing...time testing...\n",
            "time elapse: <<<0: 02: 03.00>>>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwcZDHINCA2i"
      },
      "source": [
        "import numpy as np\n",
        "import time, os\n",
        "import tensorflow as tf"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppcq5YjSDMuP"
      },
      "source": [
        "#Consider the following simple MLP with 3 layers using model subclassing:"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PcjzZrWRZGX"
      },
      "source": [
        "#We first construct our layers from scratch like as follows:\n",
        "class DenseLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, inputs_dim, *args, **kwargs):\n",
        "    super(DenseLayer, self).__init__(*args, **kwargs)\n",
        "    self.w = self.add_weight(shape = (inputs_dim, units),\n",
        "                             name = 'weights', initializer = 'random_normal',\n",
        "                             trainable = True)\n",
        "    self.b = self.add_weight(shape = (units,), initializer = 'zeros', name = 'bias',\n",
        "                             trainable = True)\n",
        "  \n",
        "  def call(self, inputs_tensor):\n",
        "    x = tf.matmul(inputs_tensor, self.w) + self.b\n",
        "    return tf.nn.relu(x)\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5owRswxfDYkn"
      },
      "source": [
        "class SimpleMLP(tf.keras.models.Model):\n",
        "  def __init__(self, num_classes = 10, *args, **kwargs):\n",
        "    super(SimpleMLP, self).__init__(*args, **kwargs)\n",
        "    self.dense1 = DenseLayer(units = 128, inputs_dim = 784, name = 'dense1')\n",
        "    self.dense2 = DenseLayer(units = 64, inputs_dim = 128, name = 'dense2')\n",
        "    self.outputs = DenseLayer(units = 10, inputs_dim = 64, name = 'outputs')\n",
        "\n",
        "  def call(self, inputs_tensor, training = False):\n",
        "    x = self.dense1(inputs_tensor)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.dense2(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    return tf.nn.softmax(self.outputs(x))"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x4sGxsjHb0g"
      },
      "source": [
        "#We can simply instantiate compile and train this model as follows:"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQqM0iFfHooT"
      },
      "source": [
        "model = SimpleMLP(name = 'MLP_CUSTOMIZED')"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5tegQj1H43r"
      },
      "source": [
        "#Load the mnist dataset:"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nK__azU-H_xN"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeWCbR6III0F",
        "outputId": "84723b63-82df-41ad-fcb9-c98acd6fa91a"
      },
      "source": [
        "print(f\"x_train_shape: {x_train.shape}, x_test_shape: {x_test.shape}\\ny_train_shape: {y_train.shape}, y_test_shape: {y_test.shape}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train_shape: (60000, 28, 28), x_test_shape: (10000, 28, 28)\n",
            "y_train_shape: (60000,), y_test_shape: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGHQVGfjIeh9"
      },
      "source": [
        "x_train, x_test = x_train.reshape((60000, 784)).astype(np.float32)/255.0, x_test.reshape((10000, 784)).astype(np.float32)/255.0"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rrHE1iQJdvg"
      },
      "source": [
        "y_train, y_test = tf.keras.utils.to_categorical(y_train, num_classes = 10), tf.keras.utils.to_categorical(y_test, num_classes = 10)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLdhAVhUJutm"
      },
      "source": [
        "#Convert to tensorflow data type:"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr8WeyFzJ3Im",
        "outputId": "66501437-c418-4e86-ee6a-6111554617fe"
      },
      "source": [
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "BUFFER = len(x_train)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.shuffle(BUFFER).batch(batch_size = BATCH_SIZE, drop_remainder = True)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_data = test_data.batch(batch_size = BATCH_SIZE, drop_remainder = True)\n",
        "x_sample_batch_train, y_sample_batch_train = next(iter(train_data))\n",
        "print(f\"x_batch_shape: {x_sample_batch_train.shape}, y_batch_shape: {y_sample_batch_train.shape}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_batch_shape: (64, 784), y_batch_shape: (64, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FdSGESULSJb"
      },
      "source": [
        "#training loop from scratch:"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUsnKftLLYbI",
        "outputId": "531a8140-927a-4149-f233-33f11eb5d6c9"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate= 0.001)\n",
        "loss_obj = tf.keras.losses.CategoricalCrossentropy(from_logits = False)\n",
        "train_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "eval_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "tic = time.time()\n",
        "for epoch in range(EPOCHS):\n",
        "  print(f\"...training start for epoch: {epoch + 1}....\\n....please wait....it may take a while.... \")\n",
        "  for (step, (x_train_batch, y_train_batch)) in enumerate(train_data):\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds = model(x_train_batch, training = True)\n",
        "      train_loss = loss_obj(y_train_batch, preds)\n",
        "    grads = tape.gradient(train_loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_metric.update_state(y_train_batch,preds)\n",
        "    train_acc = train_metric.result()\n",
        "    train_metric.reset_states()\n",
        "    if step % 200 == 0:\n",
        "      print(f\"epoch: {epoch + 1}, train accuracy: {float(train_acc):.4f}\")\n",
        "      print(f\"batch number: {step}, train loss: {float(train_loss):.4f}\")\n",
        "  for (step, (x_val_batch, y_val_batch)) in enumerate(test_data):\n",
        "    preds = model(x_val_batch, training = False)\n",
        "    val_loss = loss_obj(y_val_batch, preds)\n",
        "    eval_metric.update_state(y_val_batch, preds)\n",
        "    val_acc = eval_metric.result()\n",
        "    eval_metric.reset_states()\n",
        "    if step % 200 == 0:\n",
        "      print(f\"epoch: {epoch + 1}, validation accuracy: {float(val_acc):.4f}\")\n",
        "      print(f\"batch number: {step}, validation loss: {float(val_loss):.4f}\")\n",
        "toc = time.time()\n",
        "print(f\"\\ntotal training and evaluation time for this model is: {time_fmt(toc - tic)}\")\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...training start for epoch: 1....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 1, train accuracy: 0.0781\n",
            "batch number: 0, train loss: 2.3047\n",
            "epoch: 1, train accuracy: 0.8750\n",
            "batch number: 200, train loss: 0.4781\n",
            "epoch: 1, train accuracy: 0.7500\n",
            "batch number: 400, train loss: 0.7242\n",
            "epoch: 1, train accuracy: 0.8594\n",
            "batch number: 600, train loss: 0.4173\n",
            "epoch: 1, train accuracy: 0.8125\n",
            "batch number: 800, train loss: 0.5902\n",
            "epoch: 1, validation accuracy: 0.9531\n",
            "batch number: 0, validation loss: 0.1427\n",
            "...training start for epoch: 2....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 2, train accuracy: 0.8906\n",
            "batch number: 0, train loss: 0.3303\n",
            "epoch: 2, train accuracy: 0.9062\n",
            "batch number: 200, train loss: 0.2795\n",
            "epoch: 2, train accuracy: 0.9375\n",
            "batch number: 400, train loss: 0.2002\n",
            "epoch: 2, train accuracy: 0.8281\n",
            "batch number: 600, train loss: 0.4485\n",
            "epoch: 2, train accuracy: 0.8906\n",
            "batch number: 800, train loss: 0.3394\n",
            "epoch: 2, validation accuracy: 0.9688\n",
            "batch number: 0, validation loss: 0.0803\n",
            "...training start for epoch: 3....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 3, train accuracy: 0.8438\n",
            "batch number: 0, train loss: 0.5073\n",
            "epoch: 3, train accuracy: 0.9219\n",
            "batch number: 200, train loss: 0.2876\n",
            "epoch: 3, train accuracy: 0.9375\n",
            "batch number: 400, train loss: 0.1786\n",
            "epoch: 3, train accuracy: 0.8438\n",
            "batch number: 600, train loss: 0.3835\n",
            "epoch: 3, train accuracy: 0.9375\n",
            "batch number: 800, train loss: 0.1816\n",
            "epoch: 3, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0599\n",
            "...training start for epoch: 4....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 4, train accuracy: 0.9375\n",
            "batch number: 0, train loss: 0.1634\n",
            "epoch: 4, train accuracy: 0.7969\n",
            "batch number: 200, train loss: 0.5290\n",
            "epoch: 4, train accuracy: 0.8750\n",
            "batch number: 400, train loss: 0.2910\n",
            "epoch: 4, train accuracy: 0.8594\n",
            "batch number: 600, train loss: 0.3022\n",
            "epoch: 4, train accuracy: 0.9219\n",
            "batch number: 800, train loss: 0.2096\n",
            "epoch: 4, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0490\n",
            "...training start for epoch: 5....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 5, train accuracy: 0.8750\n",
            "batch number: 0, train loss: 0.3063\n",
            "epoch: 5, train accuracy: 0.9219\n",
            "batch number: 200, train loss: 0.1764\n",
            "epoch: 5, train accuracy: 0.8906\n",
            "batch number: 400, train loss: 0.2459\n",
            "epoch: 5, train accuracy: 0.9062\n",
            "batch number: 600, train loss: 0.1884\n",
            "epoch: 5, train accuracy: 0.8281\n",
            "batch number: 800, train loss: 0.4120\n",
            "epoch: 5, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0546\n",
            "...training start for epoch: 6....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 6, train accuracy: 0.8594\n",
            "batch number: 0, train loss: 0.3554\n",
            "epoch: 6, train accuracy: 0.8750\n",
            "batch number: 200, train loss: 0.2776\n",
            "epoch: 6, train accuracy: 0.8906\n",
            "batch number: 400, train loss: 0.2720\n",
            "epoch: 6, train accuracy: 0.9219\n",
            "batch number: 600, train loss: 0.1874\n",
            "epoch: 6, train accuracy: 0.8438\n",
            "batch number: 800, train loss: 0.3694\n",
            "epoch: 6, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0903\n",
            "...training start for epoch: 7....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 7, train accuracy: 0.9531\n",
            "batch number: 0, train loss: 0.1094\n",
            "epoch: 7, train accuracy: 0.9219\n",
            "batch number: 200, train loss: 0.2130\n",
            "epoch: 7, train accuracy: 0.8906\n",
            "batch number: 400, train loss: 0.2577\n",
            "epoch: 7, train accuracy: 0.9062\n",
            "batch number: 600, train loss: 0.2234\n",
            "epoch: 7, train accuracy: 0.9219\n",
            "batch number: 800, train loss: 0.1757\n",
            "epoch: 7, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.1050\n",
            "...training start for epoch: 8....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 8, train accuracy: 0.8594\n",
            "batch number: 0, train loss: 0.3387\n",
            "epoch: 8, train accuracy: 0.9219\n",
            "batch number: 200, train loss: 0.1722\n",
            "epoch: 8, train accuracy: 0.8750\n",
            "batch number: 400, train loss: 0.3252\n",
            "epoch: 8, train accuracy: 0.9062\n",
            "batch number: 600, train loss: 0.2214\n",
            "epoch: 8, train accuracy: 0.8906\n",
            "batch number: 800, train loss: 0.2928\n",
            "epoch: 8, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0531\n",
            "...training start for epoch: 9....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 9, train accuracy: 0.8906\n",
            "batch number: 0, train loss: 0.2773\n",
            "epoch: 9, train accuracy: 0.9531\n",
            "batch number: 200, train loss: 0.0935\n",
            "epoch: 9, train accuracy: 0.8906\n",
            "batch number: 400, train loss: 0.2566\n",
            "epoch: 9, train accuracy: 0.8906\n",
            "batch number: 600, train loss: 0.2620\n",
            "epoch: 9, train accuracy: 0.9531\n",
            "batch number: 800, train loss: 0.1378\n",
            "epoch: 9, validation accuracy: 0.9688\n",
            "batch number: 0, validation loss: 0.0800\n",
            "...training start for epoch: 10....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 10, train accuracy: 0.9062\n",
            "batch number: 0, train loss: 0.2257\n",
            "epoch: 10, train accuracy: 0.8750\n",
            "batch number: 200, train loss: 0.3683\n",
            "epoch: 10, train accuracy: 0.8594\n",
            "batch number: 400, train loss: 0.3594\n",
            "epoch: 10, train accuracy: 0.8125\n",
            "batch number: 600, train loss: 0.4283\n",
            "epoch: 10, train accuracy: 0.9062\n",
            "batch number: 800, train loss: 0.2183\n",
            "epoch: 10, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0473\n",
            "...training start for epoch: 11....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 11, train accuracy: 0.9219\n",
            "batch number: 0, train loss: 0.1805\n",
            "epoch: 11, train accuracy: 0.8281\n",
            "batch number: 200, train loss: 0.4307\n",
            "epoch: 11, train accuracy: 0.9375\n",
            "batch number: 400, train loss: 0.1571\n",
            "epoch: 11, train accuracy: 0.9062\n",
            "batch number: 600, train loss: 0.2335\n",
            "epoch: 11, train accuracy: 0.8906\n",
            "batch number: 800, train loss: 0.2661\n",
            "epoch: 11, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0960\n",
            "...training start for epoch: 12....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 12, train accuracy: 0.8438\n",
            "batch number: 0, train loss: 0.3604\n",
            "epoch: 12, train accuracy: 0.9375\n",
            "batch number: 200, train loss: 0.1218\n",
            "epoch: 12, train accuracy: 0.8750\n",
            "batch number: 400, train loss: 0.2884\n",
            "epoch: 12, train accuracy: 0.8906\n",
            "batch number: 600, train loss: 0.2610\n",
            "epoch: 12, train accuracy: 0.9062\n",
            "batch number: 800, train loss: 0.2392\n",
            "epoch: 12, validation accuracy: 0.9688\n",
            "batch number: 0, validation loss: 0.0684\n",
            "...training start for epoch: 13....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 13, train accuracy: 0.8906\n",
            "batch number: 0, train loss: 0.2672\n",
            "epoch: 13, train accuracy: 0.9062\n",
            "batch number: 200, train loss: 0.2223\n",
            "epoch: 13, train accuracy: 0.9219\n",
            "batch number: 400, train loss: 0.1877\n",
            "epoch: 13, train accuracy: 0.9062\n",
            "batch number: 600, train loss: 0.2475\n",
            "epoch: 13, train accuracy: 0.9219\n",
            "batch number: 800, train loss: 0.1746\n",
            "epoch: 13, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0445\n",
            "...training start for epoch: 14....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 14, train accuracy: 0.9062\n",
            "batch number: 0, train loss: 0.2165\n",
            "epoch: 14, train accuracy: 0.8906\n",
            "batch number: 200, train loss: 0.2729\n",
            "epoch: 14, train accuracy: 0.8438\n",
            "batch number: 400, train loss: 0.3465\n",
            "epoch: 14, train accuracy: 0.8438\n",
            "batch number: 600, train loss: 0.3619\n",
            "epoch: 14, train accuracy: 0.8750\n",
            "batch number: 800, train loss: 0.2839\n",
            "epoch: 14, validation accuracy: 0.9688\n",
            "batch number: 0, validation loss: 0.1171\n",
            "...training start for epoch: 15....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 15, train accuracy: 0.9375\n",
            "batch number: 0, train loss: 0.1471\n",
            "epoch: 15, train accuracy: 0.9219\n",
            "batch number: 200, train loss: 0.1804\n",
            "epoch: 15, train accuracy: 0.8750\n",
            "batch number: 400, train loss: 0.2918\n",
            "epoch: 15, train accuracy: 0.9062\n",
            "batch number: 600, train loss: 0.2280\n",
            "epoch: 15, train accuracy: 0.8594\n",
            "batch number: 800, train loss: 0.3249\n",
            "epoch: 15, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0702\n",
            "...training start for epoch: 16....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 16, train accuracy: 0.9062\n",
            "batch number: 0, train loss: 0.2167\n",
            "epoch: 16, train accuracy: 0.9062\n",
            "batch number: 200, train loss: 0.2273\n",
            "epoch: 16, train accuracy: 0.9375\n",
            "batch number: 400, train loss: 0.1483\n",
            "epoch: 16, train accuracy: 0.9062\n",
            "batch number: 600, train loss: 0.2168\n",
            "epoch: 16, train accuracy: 0.8750\n",
            "batch number: 800, train loss: 0.2880\n",
            "epoch: 16, validation accuracy: 0.9688\n",
            "batch number: 0, validation loss: 0.0988\n",
            "...training start for epoch: 17....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 17, train accuracy: 0.9219\n",
            "batch number: 0, train loss: 0.1844\n",
            "epoch: 17, train accuracy: 0.9062\n",
            "batch number: 200, train loss: 0.2264\n",
            "epoch: 17, train accuracy: 0.9219\n",
            "batch number: 400, train loss: 0.1801\n",
            "epoch: 17, train accuracy: 0.8906\n",
            "batch number: 600, train loss: 0.2554\n",
            "epoch: 17, train accuracy: 0.9688\n",
            "batch number: 800, train loss: 0.0758\n",
            "epoch: 17, validation accuracy: 0.9688\n",
            "batch number: 0, validation loss: 0.0711\n",
            "...training start for epoch: 18....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 18, train accuracy: 0.8438\n",
            "batch number: 0, train loss: 0.3608\n",
            "epoch: 18, train accuracy: 0.9688\n",
            "batch number: 200, train loss: 0.0724\n",
            "epoch: 18, train accuracy: 0.9375\n",
            "batch number: 400, train loss: 0.1456\n",
            "epoch: 18, train accuracy: 0.9062\n",
            "batch number: 600, train loss: 0.2169\n",
            "epoch: 18, train accuracy: 0.7500\n",
            "batch number: 800, train loss: 0.5814\n",
            "epoch: 18, validation accuracy: 0.9688\n",
            "batch number: 0, validation loss: 0.0913\n",
            "...training start for epoch: 19....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 19, train accuracy: 0.9219\n",
            "batch number: 0, train loss: 0.1868\n",
            "epoch: 19, train accuracy: 0.8594\n",
            "batch number: 200, train loss: 0.3257\n",
            "epoch: 19, train accuracy: 0.8750\n",
            "batch number: 400, train loss: 0.2885\n",
            "epoch: 19, train accuracy: 0.8906\n",
            "batch number: 600, train loss: 0.2541\n",
            "epoch: 19, train accuracy: 0.8750\n",
            "batch number: 800, train loss: 0.2741\n",
            "epoch: 19, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0862\n",
            "...training start for epoch: 20....\n",
            "....please wait....it may take a while.... \n",
            "epoch: 20, train accuracy: 0.8281\n",
            "batch number: 0, train loss: 0.3800\n",
            "epoch: 20, train accuracy: 0.9219\n",
            "batch number: 200, train loss: 0.1853\n",
            "epoch: 20, train accuracy: 0.9531\n",
            "batch number: 400, train loss: 0.1109\n",
            "epoch: 20, train accuracy: 0.8906\n",
            "batch number: 600, train loss: 0.2522\n",
            "epoch: 20, train accuracy: 0.9062\n",
            "batch number: 800, train loss: 0.2171\n",
            "epoch: 20, validation accuracy: 0.9844\n",
            "batch number: 0, validation loss: 0.0403\n",
            "\n",
            "total training and evaluation time for this model is: 0: 03: 49.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YySyQ8NAPQyO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}